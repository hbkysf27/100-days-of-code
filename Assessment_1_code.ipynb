{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbkysf27/100-days-of-code/blob/master/Assessment_1_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST7X0Vu6murY"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EtBH0_ebmBjh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import ResNetForImageClassification\n",
        "import PIL.Image\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import UnidentifiedImageError\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import PIL\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N9Jt3K-zvDbJ",
        "outputId": "0a0d933a-702e-45b9-b6f0-41c689e5821c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lf06_Kz8wiMy"
      },
      "outputs": [],
      "source": [
        "save_folder = \"/content/drive/MyDrive/a1_images\"\n",
        "os.makedirs(save_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFmTAgXf2bPu",
        "outputId": "c23b2adc-a038-403b-c646-9bcfb6907083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data_mining/Assessment_1/cs552j_A1_dataset_image_id_url.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/data_mining/Assessment_1/cs552j_A1_dataset_image_id_url.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "487us_new8sw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data_mining/Assessment_1/cs552j_A1_dataset_image_id_url.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "52gQdciqmPax"
      },
      "outputs": [],
      "source": [
        "def download_image(image_url, save_folder):\n",
        "    image_name = os.path.join(save_folder, image_url.split(\"/\")[-1])\n",
        "    response = requests.get(image_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(image_name, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Image saved at: {image_name}\")\n",
        "    else:\n",
        "        print(\"Failed to download image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mS-HElmNmSbQ",
        "outputId": "b9817ea0-bc25-4484-dc28-48fc6ba3ea47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image saved at: /content/drive/MyDrive/a1_images/000000252219.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000386912.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000308394.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000456496.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000185250.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000356427.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000266409.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000515579.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000551215.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000057597.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000399462.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000494869.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000329219.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000214720.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000459153.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000295713.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000273132.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000476415.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000292082.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000122046.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000352684.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000512836.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000177015.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000288042.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000347265.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000264535.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000337055.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000100624.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000202228.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000403565.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000441586.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000570539.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000011197.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000021167.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000297578.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000164602.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000067180.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000525155.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000253835.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000163257.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000414385.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000130586.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000349184.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000327601.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000277020.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000360960.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000567640.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000163562.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000232563.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000017436.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000571718.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000372819.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000087144.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000060347.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000013659.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000508101.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000351810.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000338624.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000439522.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000425702.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000481573.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000183675.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000105264.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000106757.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000093437.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000103548.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000568439.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000198510.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000133567.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000190140.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000032901.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000422836.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000213547.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000467511.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000297022.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000453166.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000161032.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000419974.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000054654.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000542423.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000140556.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000413395.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000427160.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000522889.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000461751.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000466986.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000229358.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000389532.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000116439.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000252716.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000061584.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000400803.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000408696.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000391375.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000432553.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000340451.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000442456.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000281929.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000257896.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000442746.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000476770.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000546976.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000277005.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000455157.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000273232.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000365642.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000396518.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000477689.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000258541.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000063047.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000489339.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000545594.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000151657.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000313454.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000269316.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000475191.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000261535.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000577932.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000389812.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000491090.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000374052.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000319534.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000049259.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000144333.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000375493.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000182805.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000291619.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000216296.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000464476.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000011149.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000393569.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000363875.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000366884.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000551794.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000094326.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000209222.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000088848.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000029397.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000389684.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000252294.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000492284.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000399764.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000151938.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000580197.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000521259.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000054931.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000249180.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000365655.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000182441.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000515025.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000568814.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000472298.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000407943.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000343803.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000308793.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000459272.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000346703.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000449909.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000172877.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000400815.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000437351.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000504074.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000200961.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000474854.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000380711.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000506707.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000350122.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000517069.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000393226.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000223188.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000203639.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000397303.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000369037.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000210388.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000022371.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000571264.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000001268.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000401446.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000377239.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000047819.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000170099.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000474095.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000222118.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000198928.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000343496.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000440171.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000227482.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000512985.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000181542.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000129756.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000127660.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000575243.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000468632.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000125257.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000431693.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000297353.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000245448.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000224724.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000511076.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000018491.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000052507.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000515077.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000404922.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000441247.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000527750.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000192607.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000549930.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000023126.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000132587.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000424975.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000260261.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000170955.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000172617.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000534270.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000529939.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000039480.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000455597.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000097988.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000463618.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000493772.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000476514.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000474293.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000098716.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000569030.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000288685.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000412286.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000147740.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000089697.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000060932.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000119088.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000096825.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000005503.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000017905.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000289594.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000237071.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000482436.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000068093.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000299887.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000394199.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000481567.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000214869.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000202339.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000459437.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000564336.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000488710.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000140203.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000443426.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000540466.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000275198.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000171788.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000163682.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000557672.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000213033.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000324158.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000076547.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000018837.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000050811.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000402774.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000570448.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000458255.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000431545.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000016249.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000294350.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000055002.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000371529.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000333402.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000295478.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000577959.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000333745.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000292456.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000013177.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000088951.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000014226.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000250137.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000248284.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000121417.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000341469.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000229553.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000504415.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000341921.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000186449.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000547336.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000322610.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000050145.jpg\n",
            "Image saved at: /content/drive/MyDrive/a1_images/000000467848.jpg\n"
          ]
        }
      ],
      "source": [
        "# downloads all the images in google colab\n",
        "\n",
        "for i in df.coco_url.values:\n",
        "    download_image(i, save_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "collapsed": true,
        "id": "_od4w74k8N2V",
        "outputId": "6545b24d-5af2-44d0-f250-efb2623f85ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  license         file_name  \\\n",
              "0           2        4  000000252219.jpg   \n",
              "1          11        1  000000386912.jpg   \n",
              "2          27        3  000000308394.jpg   \n",
              "3          28        4  000000456496.jpg   \n",
              "4          53        1  000000185250.jpg   \n",
              "\n",
              "                                            coco_url  height  width  \\\n",
              "0  http://images.cocodataset.org/val2017/00000025...     428    640   \n",
              "1  http://images.cocodataset.org/val2017/00000038...     480    640   \n",
              "2  http://images.cocodataset.org/val2017/00000030...     428    640   \n",
              "3  http://images.cocodataset.org/val2017/00000045...     426    640   \n",
              "4  http://images.cocodataset.org/val2017/00000018...     640    399   \n",
              "\n",
              "         date_captured                                         flickr_url  \\\n",
              "0  2013-11-14 22:32:02  http://farm4.staticflickr.com/3446/3232237447_...   \n",
              "1  2013-11-15 16:38:19  http://farm5.staticflickr.com/4088/4980393979_...   \n",
              "2  2013-11-17 03:48:10  http://farm4.staticflickr.com/3152/2818802025_...   \n",
              "3  2013-11-17 03:50:41  http://farm4.staticflickr.com/3810/10095484263...   \n",
              "4  2013-11-18 10:07:10  http://farm8.staticflickr.com/7040/6810762896_...   \n",
              "\n",
              "       id            label  \n",
              "0  252219  walking_running  \n",
              "1  386912          sitting  \n",
              "2  308394          sitting  \n",
              "3  456496          sitting  \n",
              "4  185250         standing  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0fcff08f-5278-47e0-b60d-7bbe2b9550f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>license</th>\n",
              "      <th>file_name</th>\n",
              "      <th>coco_url</th>\n",
              "      <th>height</th>\n",
              "      <th>width</th>\n",
              "      <th>date_captured</th>\n",
              "      <th>flickr_url</th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>000000252219.jpg</td>\n",
              "      <td>http://images.cocodataset.org/val2017/00000025...</td>\n",
              "      <td>428</td>\n",
              "      <td>640</td>\n",
              "      <td>2013-11-14 22:32:02</td>\n",
              "      <td>http://farm4.staticflickr.com/3446/3232237447_...</td>\n",
              "      <td>252219</td>\n",
              "      <td>walking_running</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>000000386912.jpg</td>\n",
              "      <td>http://images.cocodataset.org/val2017/00000038...</td>\n",
              "      <td>480</td>\n",
              "      <td>640</td>\n",
              "      <td>2013-11-15 16:38:19</td>\n",
              "      <td>http://farm5.staticflickr.com/4088/4980393979_...</td>\n",
              "      <td>386912</td>\n",
              "      <td>sitting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>000000308394.jpg</td>\n",
              "      <td>http://images.cocodataset.org/val2017/00000030...</td>\n",
              "      <td>428</td>\n",
              "      <td>640</td>\n",
              "      <td>2013-11-17 03:48:10</td>\n",
              "      <td>http://farm4.staticflickr.com/3152/2818802025_...</td>\n",
              "      <td>308394</td>\n",
              "      <td>sitting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>000000456496.jpg</td>\n",
              "      <td>http://images.cocodataset.org/val2017/00000045...</td>\n",
              "      <td>426</td>\n",
              "      <td>640</td>\n",
              "      <td>2013-11-17 03:50:41</td>\n",
              "      <td>http://farm4.staticflickr.com/3810/10095484263...</td>\n",
              "      <td>456496</td>\n",
              "      <td>sitting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>000000185250.jpg</td>\n",
              "      <td>http://images.cocodataset.org/val2017/00000018...</td>\n",
              "      <td>640</td>\n",
              "      <td>399</td>\n",
              "      <td>2013-11-18 10:07:10</td>\n",
              "      <td>http://farm8.staticflickr.com/7040/6810762896_...</td>\n",
              "      <td>185250</td>\n",
              "      <td>standing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fcff08f-5278-47e0-b60d-7bbe2b9550f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0fcff08f-5278-47e0-b60d-7bbe2b9550f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0fcff08f-5278-47e0-b60d-7bbe2b9550f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a256ddc9-994d-4ae8-8f2a-e4c27f6e0022\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a256ddc9-994d-4ae8-8f2a-e4c27f6e0022')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a256ddc9-994d-4ae8-8f2a-e4c27f6e0022 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 285,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1483,\n        \"min\": 2,\n        \"max\": 4955,\n        \"num_unique_values\": 285,\n        \"samples\": [\n          107,\n          4294,\n          2560\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"license\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          4,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 285,\n        \"samples\": [\n          \"000000057597.jpg\",\n          \"000000171788.jpg\",\n          \"000000449909.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coco_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 285,\n        \"samples\": [\n          \"http://images.cocodataset.org/val2017/000000057597.jpg\",\n          \"http://images.cocodataset.org/val2017/000000171788.jpg\",\n          \"http://images.cocodataset.org/val2017/000000449909.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": 240,\n        \"max\": 640,\n        \"num_unique_values\": 60,\n        \"samples\": [\n          428,\n          332,\n          399\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 99,\n        \"min\": 300,\n        \"max\": 640,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          436,\n          428,\n          470\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date_captured\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 285,\n        \"samples\": [\n          \"2013-11-22 02:36:28\",\n          \"2013-11-23 04:22:06\",\n          \"2013-11-16 18:57:24\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flickr_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 285,\n        \"samples\": [\n          \"http://farm7.staticflickr.com/6239/6258453198_b6ee9e6d6f_z.jpg\",\n          \"http://farm4.staticflickr.com/3293/2532994349_2338ccbf6a_z.jpg\",\n          \"http://farm5.staticflickr.com/4065/4576506981_6a421c511e_z.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 165264,\n        \"min\": 1268,\n        \"max\": 580197,\n        \"num_unique_values\": 285,\n        \"samples\": [\n          57597,\n          171788,\n          449909\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"walking_running\",\n          \"sitting\",\n          \"standing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XftXF_KjjP_H"
      },
      "source": [
        "Load dataset and extract labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "inBLhc-hAi9v"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load dataset and extract labels\n",
        "CSV_PATH = '/content/drive/MyDrive/data_mining/Assessment_1/cs552j_A1_dataset_image_id_url.csv'\n",
        "IMAGE_DIR = '/content/drive/MyDrive/a1_images'\n",
        "\n",
        "# Read CSV\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# # Use 'file_name' column directly for image file names\n",
        "df['filename'] = df['file_name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rKlof4pPGeZv"
      },
      "outputs": [],
      "source": [
        "# Use 'label' column for classification labels\n",
        "label_col = 'label'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ugySlmdIHB2y"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "label_names = df[label_col].unique()\n",
        "label_to_index = {name: idx for idx, name in enumerate(sorted(label_names))}\n",
        "df['label'] = df[label_col].map(label_to_index).astype(str)\n",
        "\n",
        "# Add full image path\n",
        "df['image_path'] = df['filename'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
        "\n",
        "# Filter only existing images\n",
        "df = df[df['image_path'].apply(os.path.exists)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "cB3i5ip-vOPp",
        "outputId": "f877080d-20d2-4e42-a9ee-a17c799428ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(285, 12)\n",
            "Total rows in CSV: 285\n",
            "Remaining after filtering existing images: 285\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    /content/drive/MyDrive/a1_images/000000252219.jpg\n",
              "1    /content/drive/MyDrive/a1_images/000000386912.jpg\n",
              "2    /content/drive/MyDrive/a1_images/000000308394.jpg\n",
              "3    /content/drive/MyDrive/a1_images/000000456496.jpg\n",
              "4    /content/drive/MyDrive/a1_images/000000185250.jpg\n",
              "Name: image_path, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/a1_images/000000252219.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/a1_images/000000386912.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/a1_images/000000308394.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/a1_images/000000456496.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/a1_images/000000185250.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "print(df.shape)\n",
        "df.head()\n",
        "print(\"Total rows in CSV:\", len(pd.read_csv(CSV_PATH)))\n",
        "print(\"Remaining after filtering existing images:\", len(df))\n",
        "df['image_path'].head()\n",
        "#!ls /content/drive/MyDrive/a1_images | head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCDQGtc9prb8"
      },
      "source": [
        "Step 3: Split into train/val/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lYHG8vD8ptNs"
      },
      "outputs": [],
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfyn2Ta9zO1"
      },
      "source": [
        " Filter out corrupted images in test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K0aGQykK9ylk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def is_valid_image(path):\n",
        "    try:\n",
        "        img = Image.open(path)\n",
        "        img.verify()\n",
        "        return True\n",
        "    except (UnidentifiedImageError, FileNotFoundError):\n",
        "        return False\n",
        "\n",
        "test_df = test_df[test_df['image_path'].apply(is_valid_image)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGnHlvEj5P0H"
      },
      "source": [
        "# ==== CNN_Base ======="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBIVgMoKxmqp"
      },
      "source": [
        "Create ImageDataGenerators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSHMqZTJxrOF",
        "outputId": "1ae4bdba-6463-4678-d9f3-00c0377c4232",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 199 validated image filenames belonging to 3 classes.\n",
            "Found 43 validated image filenames belonging to 3 classes.\n",
            "Found 43 validated image filenames belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "#target_size = (224, 224)\n",
        "\n",
        "base_gen = ImageDataGenerator(rescale=1./255)\n",
        "data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "def df_to_generator(df, shuffle=True, generator=base_gen):\n",
        "    return data_gen.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        #target_size=target_size,\n",
        "        target_size=(128, 128),\n",
        "        batch_size=32,\n",
        "        class_mode='sparse',\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "train_gen_base = df_to_generator(train_df)\n",
        "val_gen_base = df_to_generator(val_df, shuffle=False)\n",
        "test_gen_base = df_to_generator(test_df, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNWe5S7A0UxV"
      },
      "source": [
        "Define a simple CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "collapsed": true,
        "id": "wR6_OXZT0VOq",
        "outputId": "cbde5f85-697b-425c-cb2b-bc315be96807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,305,027\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,027</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,305,027\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,027</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "cnn_base = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_base.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "cnn_base.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "NZAN83a--2O6",
        "outputId": "29b6fca2-0dba-4dcd-b3db-0bbb300ea87a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.3221 - loss: 1.5949 - val_accuracy: 0.3256 - val_loss: 1.1008\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.3469 - loss: 1.0966 - val_accuracy: 0.3023 - val_loss: 1.0990\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.4417 - loss: 1.0937 - val_accuracy: 0.3256 - val_loss: 1.1025\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.3846 - loss: 1.0826 - val_accuracy: 0.3953 - val_loss: 1.1012\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.4347 - loss: 1.0734 - val_accuracy: 0.3488 - val_loss: 1.1013\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.4613 - loss: 1.0536 - val_accuracy: 0.3256 - val_loss: 1.1280\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.3827 - loss: 1.0275 - val_accuracy: 0.3023 - val_loss: 1.1078\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6431 - loss: 0.9759 - val_accuracy: 0.2791 - val_loss: 1.1780\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.6599 - loss: 0.8667 - val_accuracy: 0.2558 - val_loss: 1.2972\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.6422 - loss: 0.7991 - val_accuracy: 0.2791 - val_loss: 1.3638\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - accuracy: 0.4145 - loss: 1.2369\n",
            "CNN_base Test Accuracy: 0.42\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#simple single training and output\n",
        "history_base = cnn_base.fit(\n",
        "    train_gen_base,\n",
        "    validation_data=val_gen_base,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "test_loss_base, test_acc_base = cnn_base.evaluate(test_gen_base)\n",
        "print(f\"CNN_base Test Accuracy: {test_acc_base:.2f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_base.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_base.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_base.history['loss'], label='Train Loss')\n",
        "plt.plot(history_base.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Doing 5 training and getting average output for better estimation\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "# Run multiple trials\n",
        "n_trials = 5\n",
        "accuracies = []\n",
        "\n",
        "for i in range(n_trials):\n",
        "    print(f\"Trial {i+1}/{n_trials}\")\n",
        "\n",
        "    # Set different seed for each trial\n",
        "    set_seeds(42 + i)\n",
        "\n",
        "    # Reset and recompile the existing model\n",
        "    cnn_base.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = cnn_base.fit(\n",
        "        train_gen_base,\n",
        "        validation_data=val_gen_base,\n",
        "        epochs=10\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    _, accuracy = cnn_base.evaluate(test_gen_base)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Trial {i+1} accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Report average and standard deviation\n",
        "avg_acc = np.mean(accuracies)\n",
        "std_acc = np.std(accuracies)\n",
        "print(f\"Average accuracy: {avg_acc:.4f} ± {std_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjfPTEypdp7H",
        "outputId": "4ec9c763-54fd-41fd-9142-0e538864aeab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1/5\n",
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.7016 - loss: 0.7354 - val_accuracy: 0.2791 - val_loss: 1.3136\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.8655 - loss: 0.5799 - val_accuracy: 0.3256 - val_loss: 1.4452\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.8461 - loss: 0.4856 - val_accuracy: 0.3721 - val_loss: 1.8459\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.8647 - loss: 0.3542 - val_accuracy: 0.2326 - val_loss: 1.8593\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.9255 - loss: 0.2433 - val_accuracy: 0.2791 - val_loss: 1.9619\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.9750 - loss: 0.1556 - val_accuracy: 0.2791 - val_loss: 2.3176\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.9830 - loss: 0.0754 - val_accuracy: 0.3488 - val_loss: 2.9365\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0411 - val_accuracy: 0.2326 - val_loss: 3.1402\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.9857 - loss: 0.0471 - val_accuracy: 0.3023 - val_loss: 3.4887\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0185 - val_accuracy: 0.2791 - val_loss: 3.5950\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.4094 - loss: 2.5754\n",
            "Trial 1 accuracy: 0.3953\n",
            "Trial 2/5\n",
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9412 - loss: 0.1453 - val_accuracy: 0.3488 - val_loss: 2.0867\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.9252 - loss: 0.1991 - val_accuracy: 0.2791 - val_loss: 2.1727\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9805 - loss: 0.1138 - val_accuracy: 0.3256 - val_loss: 2.8826\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.9949 - loss: 0.0427 - val_accuracy: 0.3721 - val_loss: 2.9916\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.9971 - loss: 0.0156 - val_accuracy: 0.3721 - val_loss: 3.4087\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0099 - val_accuracy: 0.3023 - val_loss: 3.5247\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.3256 - val_loss: 3.8184\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.3023 - val_loss: 3.9804\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.3023 - val_loss: 4.1075\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.3023 - val_loss: 4.2503\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.4249 - loss: 3.0510\n",
            "Trial 2 accuracy: 0.4186\n",
            "Trial 3/5\n",
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.9112 - loss: 0.2480 - val_accuracy: 0.3721 - val_loss: 4.3439\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.9947 - loss: 0.0510 - val_accuracy: 0.3023 - val_loss: 3.0433\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0312 - val_accuracy: 0.3256 - val_loss: 3.1184\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0250 - val_accuracy: 0.3488 - val_loss: 3.1318\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0093 - val_accuracy: 0.3256 - val_loss: 3.3718\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.4186 - val_loss: 3.9343\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.3721 - val_loss: 4.0312\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.3488 - val_loss: 4.3642\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.3721 - val_loss: 4.4726\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.3721 - val_loss: 4.3740\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.3835 - loss: 3.3518\n",
            "Trial 3 accuracy: 0.3721\n",
            "Trial 4/5\n",
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9891 - loss: 0.0188 - val_accuracy: 0.3256 - val_loss: 4.2447\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.9941 - loss: 0.0125 - val_accuracy: 0.3256 - val_loss: 3.9388\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0146 - val_accuracy: 0.3721 - val_loss: 3.9170\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.3256 - val_loss: 4.3855\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.3488 - val_loss: 4.8289\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.4383e-04 - val_accuracy: 0.3721 - val_loss: 5.1725\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.1657e-04 - val_accuracy: 0.3721 - val_loss: 5.4785\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.9929e-04 - val_accuracy: 0.3488 - val_loss: 5.6043\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 8.6152e-05 - val_accuracy: 0.3488 - val_loss: 5.6809\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 6.8940e-05 - val_accuracy: 0.3721 - val_loss: 5.6501\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 0.4094 - loss: 4.8847\n",
            "Trial 4 accuracy: 0.3953\n",
            "Trial 5/5\n",
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 7.6917e-05 - val_accuracy: 0.3256 - val_loss: 9.9133\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.5995e-04 - val_accuracy: 0.3488 - val_loss: 8.6154\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.2469e-04 - val_accuracy: 0.3488 - val_loss: 8.7863\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.3256 - val_loss: 9.8067\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.9755 - loss: 0.0753 - val_accuracy: 0.2791 - val_loss: 5.7403\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.9510 - loss: 0.0904 - val_accuracy: 0.3721 - val_loss: 5.4496\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.3488 - val_loss: 7.0265\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.9889 - loss: 0.0476 - val_accuracy: 0.3488 - val_loss: 7.5206\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.3256 - val_loss: 6.4061\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.9948 - loss: 0.0305 - val_accuracy: 0.3488 - val_loss: 7.7714\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.3421 - loss: 6.5611\n",
            "Trial 5 accuracy: 0.3256\n",
            "Average accuracy: 0.3814 ± 0.0315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#improved CNN_Base (tuned1) 0.39 accuracy\n",
        "from tensorflow.keras import layers, models\n",
        "# 3. Build an improved but still basic CNN_base model\n",
        "cnn_base_improved = models.Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "# 4. Compile with standard optimizer\n",
        "cnn_base_improved.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Train for a fixed number of epochs\n",
        "history_base_improved = cnn_base_improved.fit(\n",
        "    train_gen_base,\n",
        "    validation_data=val_gen_base,\n",
        "    epochs=15\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fk6il4UkPWY6",
        "outputId": "a09cf879-c309-4e93-962b-a3511fd87333"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6s/step - accuracy: 0.3276 - loss: 2.4714 - val_accuracy: 0.3256 - val_loss: 1.0985\n",
            "Epoch 2/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 4s/step - accuracy: 0.2753 - loss: 1.1047 - val_accuracy: 0.3256 - val_loss: 1.0999\n",
            "Epoch 3/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 4s/step - accuracy: 0.3586 - loss: 1.0897 - val_accuracy: 0.3256 - val_loss: 1.1118\n",
            "Epoch 4/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.4095 - loss: 1.0727 - val_accuracy: 0.3256 - val_loss: 1.1251\n",
            "Epoch 5/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4s/step - accuracy: 0.3976 - loss: 1.0560 - val_accuracy: 0.3256 - val_loss: 1.1194\n",
            "Epoch 6/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4s/step - accuracy: 0.5701 - loss: 0.9695 - val_accuracy: 0.3488 - val_loss: 1.1564\n",
            "Epoch 7/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 4s/step - accuracy: 0.5628 - loss: 0.8971 - val_accuracy: 0.3488 - val_loss: 1.2817\n",
            "Epoch 8/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 4s/step - accuracy: 0.7465 - loss: 0.6862 - val_accuracy: 0.3256 - val_loss: 1.3104\n",
            "Epoch 9/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4s/step - accuracy: 0.8024 - loss: 0.5193 - val_accuracy: 0.4186 - val_loss: 1.5657\n",
            "Epoch 10/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4s/step - accuracy: 0.8790 - loss: 0.3297 - val_accuracy: 0.2791 - val_loss: 1.9715\n",
            "Epoch 11/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 4s/step - accuracy: 0.9387 - loss: 0.1702 - val_accuracy: 0.3953 - val_loss: 2.1480\n",
            "Epoch 12/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - accuracy: 0.9788 - loss: 0.0768 - val_accuracy: 0.3721 - val_loss: 2.6232\n",
            "Epoch 13/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4s/step - accuracy: 0.9980 - loss: 0.0239 - val_accuracy: 0.2326 - val_loss: 3.4531\n",
            "Epoch 14/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4s/step - accuracy: 0.9888 - loss: 0.0356 - val_accuracy: 0.3721 - val_loss: 3.7057\n",
            "Epoch 15/15\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4s/step - accuracy: 0.9786 - loss: 0.0423 - val_accuracy: 0.3023 - val_loss: 2.5469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1.improved CNN_Base (tuned2) 0.34accuracy\n",
        "#Use a more moderate image size\n",
        "target_size = (150, 150)  # Balance between detail and generalizability\n",
        "\n",
        "# 2. Improve base generator with some light preprocessing\n",
        "base_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    # Add very minimal augmentation to improve generalization\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "def df_to_generator(df, shuffle=True, generator=base_gen):\n",
        "    return generator.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=target_size,\n",
        "        batch_size=32,\n",
        "        class_mode='sparse',\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "train_gen_base = df_to_generator(train_df)\n",
        "val_gen_base = df_to_generator(val_df, shuffle=False)\n",
        "test_gen_base = df_to_generator(test_df, shuffle=False)\n",
        "\n",
        "# 3. Build a slightly improved CNN_base with minimal regularization\n",
        "cnn_base_improved = models.Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(150, 150, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Dense layers with minimal dropout\n",
        "    layers.Flatten(),\n",
        "    layers.Dropout(0.2),  # Light dropout to prevent severe overfitting\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "# 4. Compile with a slightly lower learning rate\n",
        "cnn_base_improved.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Train for a moderate number of epochs\n",
        "history_base_improved = cnn_base_improved.fit(\n",
        "    train_gen_base,\n",
        "    validation_data=val_gen_base,\n",
        "    epochs=20,\n",
        "    # Add basic early stopping to prevent severe overfitting\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )]\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "brI0Z4ZJWBT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1.improved CNN_Base (tuned3) 0.3953 accuracy\n",
        "# 1. Use a moderate image size\n",
        "target_size = (160, 160)  # Good balance for small datasets\n",
        "\n",
        "# 2. Simple rescaling only for the base model (no augmentation)\n",
        "base_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "def df_to_generator(df, shuffle=True, generator=base_gen):\n",
        "    return generator.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=target_size,\n",
        "        batch_size=16,  # Smaller batch size for better gradient estimates with small data\n",
        "        class_mode='sparse',\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "train_gen_base = df_to_generator(train_df)\n",
        "val_gen_base = df_to_generator(val_df, shuffle=False)\n",
        "test_gen_base = df_to_generator(test_df, shuffle=False)\n",
        "\n",
        "# 3. Build a simple, clean CNN_base model (no Input shape passing to avoid warnings)\n",
        "cnn_base = models.Sequential()\n",
        "# First convolutional block\n",
        "cnn_base.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(160, 160, 3)))\n",
        "cnn_base.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Second convolutional block\n",
        "cnn_base.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "cnn_base.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Third convolutional block\n",
        "cnn_base.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "cnn_base.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Dense layers\n",
        "cnn_base.add(layers.Flatten())\n",
        "cnn_base.add(layers.Dense(128, activation='relu'))\n",
        "cnn_base.add(layers.Dense(len(label_to_index), activation='softmax'))\n",
        "\n",
        "# 4. Compile with default Adam settings\n",
        "cnn_base.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Train for fewer epochs to avoid overfitting\n",
        "history_base = cnn_base.fit(\n",
        "    train_gen_base,\n",
        "    validation_data=val_gen_base,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# 6. Evaluate on test set\n",
        "test_loss_base, test_acc_base = cnn_base.evaluate(test_gen_base)\n",
        "print(f\"CNN_base Test Accuracy: {test_acc_base:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ng4s-VVpXwdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1.improved CNN_Base (tuned4) 0.323 accuracy\n",
        "# Combine train and validation sets for cross-validation\n",
        "combined_df = pd.concat([train_df, val_df])\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Setup 3-fold cross-validation\n",
        "n_folds = 3\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Store fold results\n",
        "fold_accuracies = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(combined_df, combined_df['label'])):\n",
        "    print(f\"Training fold {fold+1}/{n_folds}\")\n",
        "\n",
        "    # Create train and validation splits\n",
        "    train_fold = combined_df.iloc[train_idx]\n",
        "    val_fold = combined_df.iloc[val_idx]\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = df_to_generator(train_fold)\n",
        "    val_gen = df_to_generator(val_fold, shuffle=False)\n",
        "\n",
        "    # Create a simple model\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(16, (3, 3), activation='relu', input_shape=(160, 160, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(len(label_to_index), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=6\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    _, accuracy = model.evaluate(val_gen)\n",
        "    fold_accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Fold {fold+1} validation accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate average accuracy across folds\n",
        "avg_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
        "print(f\"Average validation accuracy across {n_folds} folds: {avg_accuracy:.4f}\")\n",
        "\n",
        "# Train final model on all combined data\n",
        "final_gen = df_to_generator(combined_df)\n",
        "test_gen = df_to_generator(test_df, shuffle=False)\n",
        "\n",
        "final_model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(160, 160, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "final_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "final_model.fit(final_gen, epochs=6)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = final_model.evaluate(test_gen)\n",
        "print(f\"CNN_base Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3kZiuPflZmyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Evaluate on test set\n",
        "test_loss_base, test_acc_base = cnn_base_improved.evaluate(test_gen_base)\n",
        "print(f\"CNN_base Improved Test Accuracy: {test_acc_base:.4f}\")\n",
        "\n",
        "# 7. Plot training and validation metrics\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_base_improved.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_base_improved.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_base_improved.history['loss'], label='Train Loss')\n",
        "plt.plot(history_base_improved.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 8. Generate predictions on test set for further analysis\n",
        "test_gen_base.reset()  # Reset generator before predicting\n",
        "y_pred = cnn_base_improved.predict(test_gen_base)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Get true labels\n",
        "test_gen_base.reset()  # Reset generator again\n",
        "y_true = []\n",
        "for i in range(len(test_gen_base)):\n",
        "    _, batch_labels = next(test_gen_base)\n",
        "    y_true.extend(batch_labels)\n",
        "y_true = y_true[:len(y_pred_classes)]  # Ensure same length\n",
        "\n",
        "# Create confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "class_names = [key for key, value in sorted(label_to_index.items(), key=lambda item: item[1])]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=class_names))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PujIAEuNUQPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN_Base Conclusion\n",
        "CNN_Base Conclusion\n",
        "The CNN_base model achieved an average accuracy of 41.40% (± 3.09%) across multiple trials, establishing our baseline performance for human activity classification from images. This initial implementation provided valuable insights into the challenges of training convolutional neural networks from scratch on limited datasets.\n",
        "Throughout our development process, we systematically explored several architectural variations to optimize the CNN_base model:\n",
        "\n",
        "Original implementation: Our first CNN_base model with 128×128 input resolution and three convolutional blocks (32→64→128 filters) achieved an accuracy of 44% on the test set.\n",
        "Increased resolution: When we increased the image size to 224×224, the accuracy declined to 39.53%. This suggested that higher resolution introduced more noise than useful detail for this particular task, potentially making the model more prone to overfitting.\n",
        "Added regularization techniques: Introducing light dropout (0.2) and batch normalization further reduced accuracy to 34.88%. This counter-intuitive result indicated that even minimal regularization was too constraining for the base model given our limited dataset.\n",
        "Simplified architecture: Reducing model complexity to two convolutional blocks instead of three resulted in 30.23% accuracy, demonstrating that a certain level of model capacity was necessary to capture relevant features.\n",
        "Cross-validation approach: To ensure robustness, we implemented 3-fold cross-validation, which yielded an average validation accuracy of 34.83%, confirming the general performance range.\n",
        "\n",
        "After these experiments, we reverted to our original architecture as it consistently provided the best performance. This model uses:\n",
        "\n",
        "128×128 input resolution (balancing detail capture and overfitting risk)\n",
        "Three convolutional blocks with increasing filter counts (32→64→128)\n",
        "Simple max pooling after each convolutional layer\n",
        "A single dense layer with 128 units before classification\n",
        "No regularization techniques\n",
        "\n",
        "The key observations from our multiple trials with this model include:\n",
        "\n",
        "Consistent overfitting: The model routinely achieved nearly 100% training accuracy while test accuracy remained around 41%\n",
        "Increasing validation loss: As training progressed, validation loss steadily increased, confirming memorization rather than generalization\n",
        "Initialization sensitivity: The 9% variation between trials (37.21% to 46.51%) highlighted the impact of random weight initialization on final performance\n",
        "\n",
        "We deliberately maintained this simpler architecture for our baseline model to clearly demonstrate the fundamental capabilities and limitations of a basic CNN approach. The observed overfitting will be systematically addressed in the CNN_gen model through dedicated regularization techniques, providing a clear contrast between basic and well-regularized CNN architectures for this classification task."
      ],
      "metadata": {
        "id": "ZcvKIYFCkzPZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E_20iCk5ouA"
      },
      "source": [
        "# ===== CNN_gen Step 1: Data Augmentation =====#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd13LhNp6Ewu"
      },
      "source": [
        "Image generators with augmentation for CNN_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrWdkEoS6CpA"
      },
      "outputs": [],
      "source": [
        "first argumentations 0.37\n",
        "aug_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_gen_aug = df_to_generator(train_df, generator=aug_gen)\n",
        "val_gen_aug = df_to_generator(val_df, shuffle=False)\n",
        "test_gen_aug = df_to_generator(test_df, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Second Argumentation 0.37\n",
        "aug_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],  # Vary brightness by ±20%\n",
        "    shear_range=0.1,              # Add slight shearing\n",
        "    fill_mode='nearest'           # Fill empty areas with nearest pixel values\n",
        ")\n",
        "\n",
        "train_gen_aug = df_to_generator(train_df, generator=aug_gen)\n",
        "val_gen_aug = df_to_generator(val_df, shuffle=False)  # No augmentation for validation\n",
        "test_gen_aug = df_to_generator(test_df, shuffle=False)  # No augmentation for testing"
      ],
      "metadata": {
        "id": "ZTEJTx3avb5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,           # Increased rotation\n",
        "    width_shift_range=0.25,      # Increased shift\n",
        "    height_shift_range=0.25,     # Increased shift\n",
        "    zoom_range=0.25,             # Increased zoom\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.15,            # Added shear\n",
        "    brightness_range=[0.7, 1.3], # Added brightness variation\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_gen_aug = df_to_generator(train_df, generator=aug_gen)\n",
        "val_gen_aug = df_to_generator(val_df, shuffle=False)  # No augmentation for validation\n",
        "test_gen_aug = df_to_generator(test_df, shuffle=False)  # No augmentation for testing"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nkebx_FxzcFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuxfQc9h6VZk"
      },
      "source": [
        "#Define CNN_gen model with augmentation only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbVYdbOX6Sbx"
      },
      "outputs": [],
      "source": [
        "cnn_gen_aug= models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_gen_aug.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "cnn_gen_aug.summary()\n",
        "\n",
        "history_gen_aug = cnn_gen_aug.fit(\n",
        "    train_gen_aug,\n",
        "    validation_data=val_gen_aug,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "test_loss_base, test_acc_base = cnn_base.evaluate(test_gen_aug)\n",
        "print(f\"CNN_Gen_Aug Test Accuracy: {test_acc_base:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN_base stronger argumentatio and ensembling"
      ],
      "metadata": {
        "id": "ZKjI7HghtTYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ensemble with strong augmentation\n",
        "models_list = []\n",
        "num_models = 5\n",
        "\n",
        "for i in range(num_models):\n",
        "    print(f\"Training model {i+1}/{num_models}\")\n",
        "\n",
        "    # Set different seed for each model\n",
        "    tf.random.set_seed(42 + i)\n",
        "\n",
        "    # Create model with CNN_gen architecture\n",
        "    model = models.Sequential([\n",
        "        # First convolutional block\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Second convolutional block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Third convolutional block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Flatten and dense layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(len(label_to_index), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_gen_aug, validation_data=val_gen_aug, epochs=10)\n",
        "    models_list.append(model)\n",
        "\n",
        "# Function to make ensemble predictions\n",
        "def ensemble_predict(models, test_generator):\n",
        "    # Get the number of samples and classes\n",
        "    num_samples = len(test_generator.filenames)\n",
        "    num_classes = len(label_to_index)\n",
        "    batch_size = test_generator.batch_size\n",
        "    steps = int(np.ceil(num_samples / batch_size))  # Convert to integer\n",
        "\n",
        "    # Create arrays to store predictions\n",
        "    all_preds = np.zeros((num_samples, num_classes))\n",
        "\n",
        "    # Get predictions from each model\n",
        "    for model in models:\n",
        "        test_generator.reset()  # Reset the generator before each model's prediction\n",
        "        preds = model.predict(test_generator)\n",
        "        all_preds += preds\n",
        "\n",
        "    # Average predictions\n",
        "    all_preds /= len(models)\n",
        "\n",
        "    # Get class predictions\n",
        "    class_preds = np.argmax(all_preds, axis=1)\n",
        "    return class_preds\n",
        "\n",
        "# Get ensemble predictions\n",
        "ensemble_preds = ensemble_predict(models_list, test_gen_aug)\n",
        "\n",
        "# Get true labels\n",
        "test_gen_aug.reset()\n",
        "y_true = test_gen_aug.classes[:len(ensemble_preds)]\n",
        "\n",
        "# Calculate accuracy\n",
        "ensemble_acc = accuracy_score(y_true, ensemble_preds)\n",
        "print(f\"Strong Augmentation + Ensemble Model Accuracy: {ensemble_acc:.4f}\")"
      ],
      "metadata": {
        "id": "wSOHgfAytSlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drw3UzMi6dll"
      },
      "source": [
        "# CNN_gen With Dropout and Argumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPV1TmW26cXM"
      },
      "outputs": [],
      "source": [
        "#0.44 accuracy\n",
        "cnn_gen_aug_dropout = models.Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Dropout layer added\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_gen_aug_dropout.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "cnn_gen_aug_dropout.summary()\n",
        "\n",
        "history_gen_aug_dropout = cnn_gen_aug_dropout.fit(\n",
        "    train_gen_aug,\n",
        "    validation_data=val_gen_aug,\n",
        "    epochs=15\n",
        ")\n",
        "\n",
        "test_loss_gen_aug_dropout, test_acc_gen_aug_dropout = cnn_gen_aug_dropout.evaluate(test_gen_aug)\n",
        "print(f\"CNN_gen (Aug + Dropout) Test Accuracy: {test_acc_gen_aug_dropout:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN_gen With Dropout, stronger Argumentation and ensembling"
      ],
      "metadata": {
        "id": "Y4GG5yGlqIeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train multiple models using your CNN_gen structure\n",
        "models_list = []\n",
        "num_models = 5\n",
        "\n",
        "for i in range(num_models):\n",
        "    print(f\"Training model {i+1}/{num_models}\")\n",
        "\n",
        "    # Set different seed for each model\n",
        "    tf.random.set_seed(42 + i)\n",
        "\n",
        "    # Create and train model with the CNN_gen architecture\n",
        "    model = models.Sequential([\n",
        "        # First convolutional block\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Second convolutional block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Third convolutional block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Flatten and dense layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(len(label_to_index), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(train_gen_aug, validation_data=val_gen_aug, epochs=10)\n",
        "    models_list.append(model)\n",
        "\n",
        "# Function to make ensemble predictions\n",
        "# Function to make ensemble predictions\n",
        "def ensemble_predict(models, test_generator):\n",
        "    # Get the number of samples and classes\n",
        "    num_samples = len(test_generator.filenames)\n",
        "    num_classes = len(label_to_index)\n",
        "    batch_size = test_generator.batch_size\n",
        "    steps = int(np.ceil(num_samples / batch_size))  # Convert to integer\n",
        "\n",
        "    # Create arrays to store predictions\n",
        "    all_preds = np.zeros((num_samples, num_classes))\n",
        "\n",
        "    # Get predictions from each model\n",
        "    for model in models:\n",
        "        test_generator.reset()  # Reset the generator before each model's prediction\n",
        "        preds = model.predict(test_generator)  # Don't specify steps parameter\n",
        "        all_preds += preds\n",
        "\n",
        "    # Average predictions\n",
        "    all_preds /= len(models)\n",
        "\n",
        "    # Get class predictions\n",
        "    class_preds = np.argmax(all_preds, axis=1)\n",
        "    return class_preds\n",
        "\n",
        "# Get ensemble predictions\n",
        "ensemble_preds = ensemble_predict(models_list, test_gen_aug)\n",
        "\n",
        "# Get true labels\n",
        "test_gen_aug.reset()\n",
        "y_true = test_gen_aug.classes[:len(ensemble_preds)]\n",
        "\n",
        "# Calculate accuracy\n",
        "ensemble_acc = accuracy_score(y_true, ensemble_preds)\n",
        "print(f\"Ensemble Model Accuracy: {ensemble_acc:.4f}\")"
      ],
      "metadata": {
        "id": "rT5kM5vYYE5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN_GEN with dropout only"
      ],
      "metadata": {
        "id": "Y3gvz7yQmI-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create generators without augmentation (just rescaling)\n",
        "plain_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen_plain = df_to_generator(train_df, generator=plain_gen)\n",
        "val_gen_plain = df_to_generator(val_df, shuffle=False, generator=plain_gen)\n",
        "test_gen_plain = df_to_generator(test_df, shuffle=False, generator=plain_gen)\n",
        "\n",
        "# Train ensemble with dropout but no augmentation\n",
        "models_list = []\n",
        "num_models = 5\n",
        "\n",
        "for i in range(num_models):\n",
        "    print(f\"Training model {i+1}/{num_models}\")\n",
        "\n",
        "    # Set different seed for each model\n",
        "    tf.random.set_seed(42 + i)\n",
        "\n",
        "    # Create model with dropout but using non-augmented data\n",
        "    model = models.Sequential([\n",
        "        # First convolutional block\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Second convolutional block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Third convolutional block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Flatten and dense layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(len(label_to_index), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(train_gen_plain, validation_data=val_gen_plain, epochs=10)\n",
        "    models_list.append(model)\n",
        "\n",
        "# Function to make ensemble predictions\n",
        "def ensemble_predict(models, test_generator):\n",
        "    # Get the number of samples and classes\n",
        "    num_samples = len(test_generator.filenames)\n",
        "    num_classes = len(label_to_index)\n",
        "    batch_size = test_generator.batch_size\n",
        "    steps = int(np.ceil(num_samples / batch_size))  # Convert to integer\n",
        "\n",
        "    # Create arrays to store predictions\n",
        "    all_preds = np.zeros((num_samples, num_classes))\n",
        "\n",
        "    # Get predictions from each model\n",
        "    for model in models:\n",
        "        test_generator.reset()  # Reset the generator before each model's prediction\n",
        "        preds = model.predict(test_generator)  # Don't specify steps parameter\n",
        "        all_preds += preds\n",
        "\n",
        "    # Average predictions\n",
        "    all_preds /= len(models)\n",
        "\n",
        "    # Get class predictions\n",
        "    class_preds = np.argmax(all_preds, axis=1)\n",
        "    return class_preds\n",
        "\n",
        "# Get ensemble predictions\n",
        "ensemble_preds = ensemble_predict(models_list, test_gen_plain)\n",
        "\n",
        "# Get true labels\n",
        "test_gen_plain.reset()\n",
        "y_true = test_gen_plain.classes[:len(ensemble_preds)]\n",
        "\n",
        "# Calculate accuracy\n",
        "ensemble_acc = accuracy_score(y_true, ensemble_preds)\n",
        "print(f\"Ensemble Model with Dropout Only Accuracy: {ensemble_acc:.4f}\")"
      ],
      "metadata": {
        "id": "YfQrgoYWmH-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN_Gen with dropout + early stopping + argumentation"
      ],
      "metadata": {
        "id": "nnfcGAuop0So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Then create and train the model\n",
        "cnn_gen_improved = models.Sequential([\n",
        "    # First convolutional block - more filters and 'same' padding\n",
        "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),  # Additional conv layer\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),  # Additional conv layer\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),  # Larger dense layer\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile with optimized learning rate\n",
        "cnn_gen_improved.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower learning rate\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train with data augmentation\n",
        "history_gen_improved = cnn_gen_improved.fit(\n",
        "    train_gen_aug,\n",
        "    validation_data=val_gen_aug,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss_improved, test_acc_improved = cnn_gen_improved.evaluate(test_gen_aug)\n",
        "print(f\"Improved CNN_gen Test Accuracy: {test_acc_improved:.4f}\")"
      ],
      "metadata": {
        "id": "eyPGTq2tuXzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN_gen With Dropout + Argumentation + Batch Normalization"
      ],
      "metadata": {
        "id": "ulGfD2Cr6DQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy of 0.30. The accuracy of droupout + Arguemntation is better and will proceed with both those and neglect this\n",
        "cnn_gen_aug_dropout_bn = models.Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(128, 128, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_gen_aug_dropout_bn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_gen_aug_dropout_bn = cnn_gen_aug_dropout_bn.fit(\n",
        "    train_gen_aug,\n",
        "    validation_data=val_gen_aug,\n",
        "    epochs=15\n",
        ")\n",
        "\n",
        "test_loss_bn, test_acc_bn = cnn_gen_aug_dropout_bn.evaluate(test_gen_aug)\n",
        "print(f\"CNN_gen with Augmentation, Dropout, and Batch Normalization Test Accuracy: {test_acc_bn:.4f}\")"
      ],
      "metadata": {
        "id": "5puRq1Wt00x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4MpelJkGv9X"
      },
      "source": [
        "# CNN_gen Step Dropout, Argumentation + Weight Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RJCKTFPUE3nB"
      },
      "outputs": [],
      "source": [
        "# Accuracy of 0.27. The accuracy of droupout + Arguemntation is better and will proceed with both those and neglect this\n",
        "cnn_gen_aug_dropout_l2 = models.Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(0.001),  # Add L2 regularization\n",
        "                 input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Add L2 regularization\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Add L2 regularization\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu',\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Add L2 regularization\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "# Use a lower learning rate with L2 regularization\n",
        "cnn_gen_aug_dropout_l2.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_gen_aug_dropout_l2 = cnn_gen_aug_dropout_l2.fit(\n",
        "    train_gen_aug,\n",
        "    validation_data=val_gen_aug,\n",
        "    epochs=15\n",
        ")\n",
        "\n",
        "test_loss_l2, test_acc_l2 = cnn_gen_aug_dropout_l2.evaluate(test_gen_aug)\n",
        "print(f\"CNN_gen with Augmentation, Dropout, and L2 Regularization Test Accuracy: {test_acc_l2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN_gen Step 3: Dropout, Argumentation + Early Stopping"
      ],
      "metadata": {
        "id": "p702OOPa4n0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This mode give test accuracy of 0.348\n",
        "\n",
        "cnn_gen_final = models.Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile with standard settings\n",
        "cnn_gen_final.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train with early stopping\n",
        "history_gen_final = cnn_gen_final.fit(\n",
        "    train_gen_aug,\n",
        "    validation_data=val_gen_aug,\n",
        "    epochs=20,  # Set higher max epochs, early stopping will decide when to stop\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate final model\n",
        "test_loss_final, test_acc_final = cnn_gen_final.evaluate(test_gen_aug)\n",
        "print(f\"CNN_gen Final Test Accuracy: {test_acc_final:.4f}\")"
      ],
      "metadata": {
        "id": "uI6J43g-4dqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDQpOAVoIbmG"
      },
      "source": [
        "#CNN_gen Step 4: Dropout + L2 + EarlyStopping + Argumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9gHTBY1IaJ3"
      },
      "outputs": [],
      "source": [
        "cnn_gen_aug_all = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_gen_aug_all.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "cnn_gen_aug_all.summary()\n",
        "history_gen_aug_all = cnn_gen_aug_all.fit(\n",
        "    train_gen_aug,\n",
        "    validation_data=val_gen_aug,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "test_loss_gen_aug_all, test_acc_gen_aug_all = cnn_gen_aug_all.evaluate(test_gen_aug)\n",
        "print(f\"CNN_gen (Aug + Dropout + L2 + EarlyStopping) Test Accuracy: {test_acc_gen_aug_all:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_llNA3YfM0Rk"
      },
      "source": [
        "#All the CNN-gen Accuracies\n",
        "\n",
        "CNN_base: 37.67% to 41.86% across multiple runs (avg. ~39.77%). The maximum i got  is 0.49\n",
        "\n",
        "CNN_gen with Dropout-only (Ensemble): 32.56% to 41.86% across runs\n",
        "\n",
        "CNN_gen with Strong Augmentation + Ensemble: 32.56%\n",
        "\n",
        "CNN_gen With Dropout and Strong Argumentation 35%\n",
        "\n",
        "CNN_gen With Dropout, stronger Argumentation and ensembling 30%\n",
        "\n",
        "CNN_GEN with dropout + Ensemble : 32.5%\n",
        "\n",
        "CNN_Gen with dropout + early stopping + strong argumentation : 39.5%\n",
        "\n",
        "CNN_gen With Dropout + Strong Argumentation + Batch Normalization 34%\n",
        "\n",
        "CNN_gen Step Dropout, Strong Argumentation + Weight Decay 32.5%\n",
        "\n",
        "CNN_gen Step 3: Dropout, Strong  Argumentation + Early Stopping 37.2%\n",
        "\n",
        "CNN_gen Strong  Argumentation + Dropout + L2 + EarlyStopping) Test Accuracy: 44%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN_Gen Conclusion\n",
        "Building upon our CNN_base findings, we systematically explored various generalization techniques to address the overfitting observed in our baseline model. The CNN_gen experiments provided important insights into the effectiveness of different regularization approaches when working with limited training data.\n",
        "Through extensive experimentation, we tested numerous combinations of generalization techniques:\n",
        "\n",
        "Data augmentation: We implemented increasingly strong augmentation strategies, from basic transformations (rotation=20°, width/height shifts=0.1) to more aggressive variations (rotation=30°, width/height shifts=0.25, zoom=0.25, brightness adjustments, and shear). Data augmentation alone achieved approximately 37% accuracy.\n",
        "Dropout regularization: We applied dropout after each pooling layer (0.25) and before the output layer (0.5) to prevent co-adaptation of neurons. The dropout-only approach achieved 32.56-41.86% accuracy across different runs.\n",
        "Ensemble methods: We created ensembles of 5 models with different random initializations to average predictions and reduce variance. Ensemble approaches showed varying results depending on the underlying model architecture.\n",
        "Combined approaches: We tested numerous combinations of techniques, including:\n",
        "\n",
        "Dropout + Strong Augmentation: 35%\n",
        "Dropout + Strong Augmentation + Ensemble: 30-32.56%\n",
        "Dropout + Strong Augmentation + Batch Normalization: 34%\n",
        "Dropout + Strong Augmentation + Weight Decay: 32.5%\n",
        "Dropout + Strong Augmentation + Early Stopping: 37.2-39.5%\n",
        "Dropout + Strong Augmentation + L2 + Early Stopping: 44%\n",
        "\n",
        "\n",
        "\n",
        "The most effective configuration was the combination of dropout, strong augmentation, L2 regularization, and early stopping, which achieved 44% accuracy. This represents only a modest improvement over our baseline CNN_base model (which achieved up to 49% in some trials), highlighting the fundamental challenge of training CNNs from scratch on small datasets.\n",
        "Our experiments revealed several key insights:\n",
        "\n",
        "Diminishing returns from regularization: Adding more regularization techniques did not necessarily improve performance and often reduced accuracy, suggesting that excessive constraints can prevent models from learning useful patterns from limited data.\n",
        "Consistent overfitting patterns: Despite our regularization efforts, most models still showed signs of overfitting, with training accuracy significantly exceeding test accuracy, though to a lesser extent than the baseline model.\n",
        "High variance across trials: Performance varied considerably between runs with identical architectures but different random initializations, demonstrating the sensitivity of neural networks when trained on small datasets.\n",
        "Impact of early stopping: Early stopping proved beneficial in preventing severe overfitting during extended training, helping to preserve model generalization.\n",
        "\n",
        "The CNN_gen experiments demonstrate that while regularization techniques can somewhat mitigate overfitting, they cannot fundamentally overcome the limitations of training complex models on small datasets (approximately 300 images across 3 classes). This finding helps explain why transfer learning and multimodal approaches, which leverage knowledge from millions of pre-trained images, achieve dramatically better performance on this task."
      ],
      "metadata": {
        "id": "1Zxk4FImKxlx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQy8t5M6BVMH"
      },
      "source": [
        "#Transfer Learning using MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YUTWhCJBU4j"
      },
      "outputs": [],
      "source": [
        "\"\"\"from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOv125BmBkjn"
      },
      "source": [
        "Select two classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djGAic1qBhM1"
      },
      "outputs": [],
      "source": [
        "# Choose two classes for binary classification\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "\n",
        "# Convert label names to their encoded string versions\n",
        "binary_class_ids = [str(label_to_index[c]) for c in binary_classes]\n",
        "\n",
        "# Filter dataset to only include selected classes\n",
        "df_binary = df[df['label'].isin(binary_class_ids)].copy()\n",
        "\n",
        "# Map the encoded label strings to binary values 0 and 1\n",
        "class_map = {binary_class_ids[0]: 0, binary_class_ids[1]: 1}\n",
        "df_binary['label'] = df_binary['label'].map(class_map).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acxtLPoIJzHB"
      },
      "outputs": [],
      "source": [
        "train_bin_df, temp_bin_df = train_test_split(df_binary, test_size=0.3, stratify=df_binary['label'], random_state=42)\n",
        "val_bin_df, test_bin_df = train_test_split(temp_bin_df, test_size=0.5, stratify=temp_bin_df['label'], random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0mNSjsqLRlr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V9PS9ytYLSA8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers timm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FMyp7aqRXiQ"
      },
      "source": [
        "HuggingFace Dataset + Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYvxsrhcRW6h"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df, processor):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = PIL.Image.open(row['image_path']).convert(\"RGB\")\n",
        "        label = int(row['label'])\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        return {\"pixel_values\": inputs['pixel_values'].squeeze(), \"labels\": torch.tensor(label)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqOKHPezRgiA"
      },
      "source": [
        "HuggingFace processor and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XD3ZSY9eRkqp"
      },
      "outputs": [],
      "source": [
        "#from transformers import AutoImageProcessor\n",
        "#from transformers import ResNetForImageClassification\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n",
        "model = ResNetForImageClassification.from_pretrained(\n",
        "    \"microsoft/resnet-18\",\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True  # <-- fixes the mismatch issue\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U0HhQKCRq93"
      },
      "source": [
        "Create datasets and loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI_yN61eRmju"
      },
      "outputs": [],
      "source": [
        "train_ds = CustomImageDataset(train_bin_df, processor)\n",
        "val_ds = CustomImageDataset(val_bin_df, processor)\n",
        "test_ds = CustomImageDataset(test_bin_df, processor)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=8)\n",
        "test_loader = DataLoader(test_ds, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YCYsnRtRuge"
      },
      "source": [
        "Train Binary Classifier (ResNet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OiDGSR8RwiZ"
      },
      "outputs": [],
      "source": [
        "#import PIL.Image\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, correct = 0, 0\n",
        "    for batch in train_loader:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}, Train Accuracy: {acc:.2f}, Loss: {train_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u2gHKuURzM6"
      },
      "source": [
        "Evaluate Binary Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV9L-1WhR01Y"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(pixel_values=pixel_values)\n",
        "        preds = outputs.logits.argmax(dim=1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import accuracy_score\n",
        "\n",
        "transfer_val_acc = accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "8wJjlcc04ULZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Improved Transfer Learning Model Using (ResNet18)"
      ],
      "metadata": {
        "id": "xG7Mn25k-J6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose two classes for binary classification\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "\n",
        "# Convert label names to their encoded string versions\n",
        "binary_class_ids = [str(label_to_index[c]) for c in binary_classes]\n",
        "\n",
        "# Filter dataset to only include selected classes\n",
        "df_binary = df[df['label'].isin(binary_class_ids)].copy()\n",
        "\n",
        "# Map the encoded label strings to binary values 0 and 1\n",
        "class_map = {binary_class_ids[0]: 0, binary_class_ids[1]: 1}\n",
        "df_binary['label'] = df_binary['label'].map(class_map).astype(int)\n",
        "\n",
        "# Split the binary data\n",
        "train_bin_df, temp_bin_df = train_test_split(df_binary, test_size=0.3, stratify=df_binary['label'], random_state=42)\n",
        "val_bin_df, test_bin_df = train_test_split(temp_bin_df, test_size=0.5, stratify=temp_bin_df['label'], random_state=42)\n",
        "\n",
        "# Fixed dataset class that consistently uses the processor\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df, processor):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = PIL.Image.open(row['image_path']).convert(\"RGB\")\n",
        "        label = int(row['label'])\n",
        "\n",
        "        # Use processor consistently for all images\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].squeeze()\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": torch.tensor(label)}\n",
        "\n",
        "# Create processor with data augmentation for training\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "# Load processor and model\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n",
        "model = ResNetForImageClassification.from_pretrained(\n",
        "    \"microsoft/resnet-18\",\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Implement two-stage training: First freeze early layers, then unfreeze\n",
        "# Freeze all layers except the classifier\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Create datasets\n",
        "train_ds = CustomImageDataset(train_bin_df, processor)\n",
        "val_ds = CustomImageDataset(val_bin_df, processor)\n",
        "test_ds = CustomImageDataset(test_bin_df, processor)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16)\n",
        "test_loader = DataLoader(test_ds, batch_size=16)\n",
        "\n",
        "# Configure optimizer with learning rate scheduler\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=2e-4,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
        ")\n",
        "\n",
        "# Training with early stopping\n",
        "best_val_acc = 0\n",
        "patience = 3\n",
        "counter = 0\n",
        "EPOCHS = 15\n",
        "best_model_path = \"best_binary_model.pt\"\n",
        "\n",
        "print(\"Stage 1: Training only the classifier\")\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss, train_correct = 0, 0\n",
        "    for batch in train_loader:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            val_correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}, Loss: {train_loss:.2f}\")\n",
        "\n",
        "    # Update learning rate based on validation accuracy\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# After initial training, unfreeze all layers and continue with lower learning rate\n",
        "print(\"\\nStage 2: Fine-tuning the entire model\")\n",
        "# Load best model from first stage\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "# Unfreeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Lower learning rate for fine-tuning\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-5,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
        ")\n",
        "\n",
        "# Reset best val accuracy\n",
        "best_val_acc = 0\n",
        "counter = 0\n",
        "final_model_path = \"best_binary_model_finetuned.pt\"\n",
        "\n",
        "# Continue training\n",
        "for epoch in range(10):  # More epochs for fine-tuning\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss, train_correct = 0, 0\n",
        "    for batch in train_loader:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            val_correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Fine-tuning Epoch {epoch+1}, Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}, Loss: {train_loss:.2f}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), final_model_path)\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping fine-tuning at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# Load the best model for final evaluation\n",
        "try:\n",
        "    model.load_state_dict(torch.load(final_model_path))\n",
        "except:\n",
        "    # If fine-tuning didn't complete or improve, use the stage 1 model\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    print(\"Using stage 1 model for final evaluation\")\n",
        "\n",
        "# Final evaluation\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(pixel_values=pixel_values)\n",
        "        preds = outputs.logits.argmax(dim=1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=binary_classes))"
      ],
      "metadata": {
        "id": "nZ6TNKII4kVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An accuracy of 79.31% is a significant improvement over your previous 72% result. This improvement demonstrates the effectiveness of the transfer learning approach with proper optimization techniques.\n",
        "The key factors that likely contributed to this improvement include:\n",
        "\n",
        "The two-stage training approach (freezing then unfreezing) that allows the model to adapt to your specific task without losing the knowledge in the pretrained weights\n",
        "Proper learning rate scheduling that dynamically adjusted the learning rate based on validation performance\n",
        "The early stopping mechanism that prevented overfitting by monitoring validation accuracy\n",
        "Weight decay regularization that helped the model generalize better\n",
        "\n",
        "This result aligns perfectly with your assessment progression:\n",
        "\n",
        "CNN_base: ~40% accuracy\n",
        "CNN_gen (with generalization techniques): ~44% accuracy\n",
        "Transfer learning: 79.31% accuracy\n",
        "\n",
        "This clear progression illustrates one of the most important principles in modern deep learning: when working with limited datasets, leveraging knowledge from pretrained models (transfer learning) often yields substantially better results than training from scratch, even with sophisticated regularization techniques.\n",
        "For your report, you can highlight how this binary classifier successfully distinguishes between \"walking_running\" and \"standing\" activities with nearly 80% accuracy, despite the limited training data available."
      ],
      "metadata": {
        "id": "SwAVA_sF-Cuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transfer Learning using tensorflow"
      ],
      "metadata": {
        "id": "RGnCItwS1PWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer Learning in TensorFlow (v2)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# === 1. Recreate label_name column if not present ===\n",
        "if 'label_name' not in df.columns:\n",
        "    reverse_label_map = {v: k for k, v in label_to_index.items()}\n",
        "    df['label_name'] = df['label'].astype(int).map(reverse_label_map)\n",
        "\n",
        "# === 2. Filter and encode binary classes ===\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "df_binary_tf = df[df['label_name'].isin(binary_classes)].copy()\n",
        "df_binary_tf['label'] = df_binary_tf['label_name'].map({binary_classes[0]: 0, binary_classes[1]: 1})\n",
        "\n",
        "# === 3. Train/Val/Test split ===\n",
        "train_df_tf, temp_df_tf = train_test_split(df_binary_tf, test_size=0.3, stratify=df_binary_tf['label'], random_state=42)\n",
        "val_df_tf, test_df_tf = train_test_split(temp_df_tf, test_size=0.5, stratify=temp_df_tf['label'], random_state=42)\n",
        "\n",
        "# === 4. ImageDataGenerators ===\n",
        "img_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen_tf = img_gen.flow_from_dataframe(train_df_tf, x_col='image_path', y_col='label',\n",
        "                                           target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "val_gen_tf = img_gen.flow_from_dataframe(val_df_tf, x_col='image_path', y_col='label',\n",
        "                                         target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "test_gen_tf = img_gen.flow_from_dataframe(test_df_tf, x_col='image_path', y_col='label',\n",
        "                                          target_size=(128, 128), class_mode='raw', batch_size=32, shuffle=False)\n",
        "\n",
        "# === 5. Transfer Learning Model ===\n",
        "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
        "base_model.trainable = False  # Freeze backbone\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
        "\n",
        "model_tf = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model_tf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# === 6. Train ===\n",
        "history_tf = model_tf.fit(train_gen_tf, validation_data=val_gen_tf, epochs=5)\n",
        "\n",
        "# === 7. Evaluate ===\n",
        "preds = model_tf.predict(test_gen_tf).flatten()\n",
        "preds_binary = (preds > 0.5).astype(int)\n",
        "true_labels = test_gen_tf.labels\n",
        "\n",
        "acc_tf = accuracy_score(true_labels, preds_binary)\n",
        "print(f\"Test Accuracy (TensorFlow): {acc_tf:.2f}\")\n",
        "print(classification_report(true_labels, preds_binary))\n"
      ],
      "metadata": {
        "id": "Q6p_qS3jV2S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TensorFlow Transfer Learning - Improved Version\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# === 1. Filter and encode binary classes ===\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "df_binary_tf = df[df['label_name'].isin(binary_classes)].copy()\n",
        "df_binary_tf['label'] = df_binary_tf['label_name'].map({binary_classes[0]: 0, binary_classes[1]: 1})\n",
        "\n",
        "# === 2. Train/Val/Test split ===\n",
        "train_df_tf, temp_df_tf = train_test_split(df_binary_tf, test_size=0.3, stratify=df_binary_tf['label'], random_state=42)\n",
        "val_df_tf, test_df_tf = train_test_split(temp_df_tf, test_size=0.5, stratify=temp_df_tf['label'], random_state=42)\n",
        "\n",
        "# === 3. ImageDataGenerators with MobileNetV2 preprocessing ===\n",
        "img_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_gen_tf = img_gen.flow_from_dataframe(train_df_tf, x_col='image_path', y_col='label',\n",
        "                                           target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "val_gen_tf = img_gen.flow_from_dataframe(val_df_tf, x_col='image_path', y_col='label',\n",
        "                                         target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "test_gen_tf = img_gen.flow_from_dataframe(test_df_tf, x_col='image_path', y_col='label',\n",
        "                                          target_size=(128, 128), class_mode='raw', batch_size=32, shuffle=False)\n",
        "\n",
        "# === 4. Transfer Learning Model with partial fine-tuning ===\n",
        "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
        "\n",
        "# Unfreeze last 20 layers\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
        "\n",
        "model_tf = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model_tf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# === 5. Train ===\n",
        "history_tf = model_tf.fit(train_gen_tf, validation_data=val_gen_tf, epochs=10)\n",
        "\n",
        "# === 6. Evaluate ===\n",
        "loss, acc = model_tf.evaluate(test_gen_tf)\n",
        "print(f\"Test Accuracy (TensorFlow Improved): {acc:.2f}\")"
      ],
      "metadata": {
        "id": "-P5gfN-Kj1m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ✅ Advanced Transfer Learning in TensorFlow (Improved Accuracy)\n",
        "\n",
        "#python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# === 1. Filter and encode binary classes ===\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "df_binary_tf = df[df['label_name'].isin(binary_classes)].copy()\n",
        "df_binary_tf['label'] = df_binary_tf['label_name'].map({binary_classes[0]: 0, binary_classes[1]: 1})\n",
        "\n",
        "# === 2. Train/Val/Test split ===\n",
        "train_df_tf, temp_df_tf = train_test_split(df_binary_tf, test_size=0.3, stratify=df_binary_tf['label'], random_state=42)\n",
        "val_df_tf, test_df_tf = train_test_split(temp_df_tf, test_size=0.5, stratify=temp_df_tf['label'], random_state=42)\n",
        "\n",
        "# === 3. ImageDataGenerators with Augmentation ===\n",
        "img_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "train_gen_tf = img_gen.flow_from_dataframe(train_df_tf, x_col='image_path', y_col='label',\n",
        "                                           target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "val_gen_tf = img_gen.flow_from_dataframe(val_df_tf, x_col='image_path', y_col='label',\n",
        "                                         target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "test_gen_tf = img_gen.flow_from_dataframe(test_df_tf, x_col='image_path', y_col='label',\n",
        "                                          target_size=(128, 128), class_mode='raw', batch_size=32, shuffle=False)\n",
        "\n",
        "# === 4. Transfer Learning Model (EfficientNetB0) ===\n",
        "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
        "base_model.trainable = False  # Freeze base initially\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
        "\n",
        "model_tf = Model(inputs=base_model.input, outputs=output)\n",
        "model_tf.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# === 5. Train with EarlyStopping ===\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "history_tf = model_tf.fit(train_gen_tf, validation_data=val_gen_tf, epochs=20, callbacks=[early_stop])\n",
        "\n",
        "# === 6. Fine-tuning: unfreeze top layers ===\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_tf.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_fine = model_tf.fit(train_gen_tf, validation_data=val_gen_tf, epochs=10, callbacks=[early_stop])\n",
        "\n",
        "# === 7. Evaluate ===\n",
        "loss, acc = model_tf.evaluate(test_gen_tf)\n",
        "print(f\"Test Accuracy (TensorFlow Improved): {acc:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ysR0VECZlfDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 📦 TensorFlow Transfer Learning - Optimized Version (MobileNetV2)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# === 1. Prepare Data ===\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "df_binary_tf = df[df['label_name'].isin(binary_classes)].copy()\n",
        "df_binary_tf['label'] = df_binary_tf['label_name'].map({binary_classes[0]: 0, binary_classes[1]: 1})\n",
        "\n",
        "# Train/Val/Test Split\n",
        "train_df_tf, temp_df_tf = train_test_split(df_binary_tf, test_size=0.3, stratify=df_binary_tf['label'], random_state=42)\n",
        "val_df_tf, test_df_tf = train_test_split(temp_df_tf, test_size=0.5, stratify=temp_df_tf['label'], random_state=42)\n",
        "\n",
        "# === 2. Data Augmentation ===\n",
        "img_gen_aug = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# No augmentation for val/test\n",
        "img_gen_plain = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen_tf = img_gen_aug.flow_from_dataframe(train_df_tf, x_col='image_path', y_col='label',\n",
        "                                               target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "val_gen_tf = img_gen_plain.flow_from_dataframe(val_df_tf, x_col='image_path', y_col='label',\n",
        "                                               target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "test_gen_tf = img_gen_plain.flow_from_dataframe(test_df_tf, x_col='image_path', y_col='label',\n",
        "                                                target_size=(128, 128), class_mode='raw', batch_size=32, shuffle=False)\n",
        "\n",
        "# === 3. MobileNetV2 Model ===\n",
        "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
        "\n",
        "# Freeze all but the top 15 layers\n",
        "for layer in base_model.layers[:-15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_tf = Model(inputs=base_model.input, outputs=output)\n",
        "model_tf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# === 4. Callbacks ===\n",
        "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "# === 5. Train ===\n",
        "history_tf = model_tf.fit(train_gen_tf, validation_data=val_gen_tf, epochs=20, callbacks=[early_stop])\n",
        "\n",
        "# === 6. Evaluate ===\n",
        "loss, acc = model_tf.evaluate(test_gen_tf)\n",
        "print(f\"Test Accuracy (TensorFlow Optimized MobileNetV2): {acc:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0QD7HTw-m-ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Final Optimized Transfer Learning Model (TensorFlow - MobileNetV2)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# === 1. Filter binary classes (label_name already restored earlier) ===\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "df_binary_tf = df[df['label_name'].isin(binary_classes)].copy()\n",
        "df_binary_tf['label'] = df_binary_tf['label_name'].map({binary_classes[0]: 0, binary_classes[1]: 1})\n",
        "\n",
        "# === 2. Train/Val/Test split ===\n",
        "train_df_tf, temp_df_tf = train_test_split(df_binary_tf, test_size=0.3, stratify=df_binary_tf['label'], random_state=42)\n",
        "val_df_tf, test_df_tf = train_test_split(temp_df_tf, test_size=0.5, stratify=temp_df_tf['label'], random_state=42)\n",
        "\n",
        "# === 3. Data Generators with augmentation for training ===\n",
        "train_aug = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_aug = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen_tf = train_aug.flow_from_dataframe(train_df_tf, x_col='image_path', y_col='label',\n",
        "                                             target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "val_gen_tf = val_test_aug.flow_from_dataframe(val_df_tf, x_col='image_path', y_col='label',\n",
        "                                              target_size=(128, 128), class_mode='raw', batch_size=32)\n",
        "test_gen_tf = val_test_aug.flow_from_dataframe(test_df_tf, x_col='image_path', y_col='label',\n",
        "                                               target_size=(128, 128), class_mode='raw', batch_size=32, shuffle=False)\n",
        "\n",
        "# === 4. Load MobileNetV2 with trainable layers ===\n",
        "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
        "\n",
        "# Unfreeze last 50% of layers\n",
        "for layer in base_model.layers[:len(base_model.layers) // 2]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[len(base_model.layers) // 2:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# === 5. Construct Model ===\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_tf_final = Model(inputs=base_model.input, outputs=out)\n",
        "\n",
        "model_tf_final.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                       loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# === 6. Callbacks ===\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1)\n",
        "]\n",
        "\n",
        "# === 7. Train ===\n",
        "history_tf_final = model_tf_final.fit(train_gen_tf, validation_data=val_gen_tf, epochs=15, callbacks=callbacks)\n",
        "\n",
        "# === 8. Evaluate ===\n",
        "loss, acc = model_tf_final.evaluate(test_gen_tf)\n",
        "print(f\"Test Accuracy (TensorFlow Optimized MobileNetV2): {acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "oR_zBAt5ns3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFTa_lQsZk3g"
      },
      "source": [
        "#==== FNN_Base ===="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_innkiDRDAM2"
      },
      "outputs": [],
      "source": [
        "#from tensorflow.keras.utils import to_categorical\n",
        "#from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X29C9tljaZPW"
      },
      "source": [
        "Function to load and flatten images for FNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgHO98wlaXM_"
      },
      "outputs": [],
      "source": [
        "#from PIL import UnidentifiedImageError\n",
        "\n",
        "def load_flattened_images(df):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i, row in df.iterrows():\n",
        "        try:\n",
        "            img = load_img(row['image_path'], target_size=(128, 128))\n",
        "            arr = img_to_array(img) / 255.0\n",
        "            X.append(arr.flatten())\n",
        "            y.append(int(row['label']))\n",
        "        except (UnidentifiedImageError, OSError):\n",
        "            print(f\"Skipping unreadable image: {row['image_path']}\")\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_train_fnn, y_train_fnn = load_flattened_images(train_df)\n",
        "X_val_fnn, y_val_fnn = load_flattened_images(val_df)\n",
        "X_test_fnn, y_test_fnn = load_flattened_images(test_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfkBkFbafrDS"
      },
      "source": [
        "Define and train the FNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DZtiteJRfrgq"
      },
      "outputs": [],
      "source": [
        "fnn_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(128*128*3,)),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])\n",
        "\n",
        "fnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "fnn_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm3GRzxyf1sh"
      },
      "source": [
        "Train FNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WK9CP1WQf2Gl"
      },
      "outputs": [],
      "source": [
        "history_fnn = fnn_model.fit(\n",
        "    X_train_fnn, y_train_fnn,\n",
        "    validation_data=(X_val_fnn, y_val_fnn),\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDCp3fhgf8kI"
      },
      "source": [
        "Evaluate FNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TIu3ii1lf-Ve"
      },
      "outputs": [],
      "source": [
        "test_loss_fnn, test_acc_fnn = fnn_model.evaluate(X_test_fnn, y_test_fnn)\n",
        "print(f\"FNN Test Accuracy: {test_acc_fnn:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N4n3Cm3gCPS"
      },
      "source": [
        "Plot accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HAlVMrEcgCiN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_fnn.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_fnn.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('FNN Training & Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_fnn.history['loss'], label='Train Loss')\n",
        "plt.plot(history_fnn.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('FNN Training & Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etsLGgFpKQP8"
      },
      "source": [
        "# Step 4: Multimodal Embeddings 1 (CLIP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZddQITINkSn"
      },
      "source": [
        "Untuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kWhMHJ9DKSb7"
      },
      "outputs": [],
      "source": [
        "\"\"\"#Load CLIP Processor and Model (Image Encoder Only)\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "clip_model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model.to(device)\n",
        "\n",
        "#Extract Image Embeddings using CLIP\n",
        "\n",
        "def extract_clip_embeddings(df):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    for idx, row in df.iterrows():\n",
        "        image = PIL.Image.open(row['image_path']).convert(\"RGB\")\n",
        "        inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.get_image_features(**inputs)\n",
        "        embeddings.append(image_features.cpu().squeeze().numpy())\n",
        "        labels.append(int(row['label']))\n",
        "    return np.array(embeddings), np.array(labels)\n",
        "\n",
        "X_train_clip, y_train_clip = extract_clip_embeddings(train_df)\n",
        "X_val_clip, y_val_clip = extract_clip_embeddings(val_df)\n",
        "X_test_clip, y_test_clip = extract_clip_embeddings(test_df)\n",
        "\n",
        "#Build Simple FNN Classifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "clip_fnn_model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(512,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(len(np.unique(y_train_clip)), activation='softmax')\n",
        "])\n",
        "\n",
        "clip_fnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Train FNN on CLIP Embeddings\n",
        "history_clip_fnn = clip_fnn_model.fit(\n",
        "    X_train_clip, y_train_clip,\n",
        "    validation_data=(X_val_clip, y_val_clip),\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "#Evaluate\n",
        "test_loss_clip, test_acc_clip = clip_fnn_model.evaluate(X_test_clip, y_test_clip)\n",
        "print(f\"CLIP Embedding Classifier Test Accuracy: {test_acc_clip:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMHoRGlNn_k"
      },
      "source": [
        "Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'label' to int\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# Reverse the label_to_index mapping\n",
        "reverse_label_map = {v: k for k, v in label_to_index.items()}\n",
        "\n",
        "# Recreate 'label_name'\n",
        "df['label_name'] = df['label'].map(reverse_label_map)\n",
        "\n",
        "# Check again\n",
        "print(\"Unique label names:\", df['label_name'].unique())\n"
      ],
      "metadata": {
        "id": "kbRVvJZlkBBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create df_binary (filtered for binary classification)\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "\n",
        "# Reverse the label map if needed (only if you don't already have encoded labels)\n",
        "if 'label_name' not in df.columns:\n",
        "    reverse_label_map = {v: k for k, v in label_to_index.items()}\n",
        "    df['label_name'] = df['label'].map(reverse_label_map)\n",
        "\n",
        "# Filter for binary classes\n",
        "df_binary = df[df['label_name'].isin(binary_classes)].copy()\n",
        "\n",
        "# Encode binary classes to 0 and 1\n",
        "df_binary['label'] = df_binary['label_name'].map({binary_classes[0]: 0, binary_classes[1]: 1}).astype(int)\n"
      ],
      "metadata": {
        "id": "OLpx_jb8jfgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5XMOCNGDNpgG"
      },
      "outputs": [],
      "source": [
        "# Step 4: Multimodal Embeddings 1 (CLIP)\n",
        "\n",
        "#from transformers import CLIPProcessor, CLIPModel\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from tensorflow.keras import layers, models\n",
        "#import tensorflow as tf\n",
        "#from sklearn.metrics import classification_report\n",
        "#import numpy as np\n",
        "\n",
        "# Load CLIP model and processor\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Prepare data\n",
        "clip_binary_df = df_binary.copy()\n",
        "clip_train_df, temp_df = train_test_split(clip_binary_df, test_size=0.3, stratify=clip_binary_df['label'], random_state=42)\n",
        "clip_val_df, clip_test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "# Function to extract image embeddings from CLIP\n",
        "from PIL import Image\n",
        "\n",
        "def get_clip_embeddings(df_subset):\n",
        "    embeddings = []\n",
        "    for path in df_subset['image_path']:\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.get_image_features(**inputs)\n",
        "        embeddings.append(image_features[0].numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Extract embeddings\n",
        "clip_train_embeddings = get_clip_embeddings(clip_train_df)\n",
        "clip_val_embeddings = get_clip_embeddings(clip_val_df)\n",
        "clip_test_embeddings = get_clip_embeddings(clip_test_df)\n",
        "\n",
        "# Get binary labels\n",
        "y_train_bin = clip_train_df['label'].astype(int).values\n",
        "y_val_bin = clip_val_df['label'].astype(int).values\n",
        "y_test_bin = clip_test_df['label'].astype(int).values\n",
        "\n",
        "# Build a tuned FNN classifier\n",
        "clip_classifier = models.Sequential([\n",
        "    layers.Input(shape=(clip_train_embeddings.shape[1],)),\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "clip_classifier.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history_clip = clip_classifier.fit(\n",
        "    clip_train_embeddings, y_train_bin,\n",
        "    validation_data=(clip_val_embeddings, y_val_bin),\n",
        "    epochs=20,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss_clip, test_acc_clip = clip_classifier.evaluate(clip_test_embeddings, y_test_bin)\n",
        "print(f\"\\n\\033[1mCLIP Embedding Classifier Test Accuracy (Tuned): {test_acc_clip:.2f}\\033[0m\")\n",
        "\n",
        "# Classification report\n",
        "y_pred_clip = (clip_classifier.predict(clip_test_embeddings) > 0.5).astype(int)\n",
        "print(classification_report(y_test_bin, y_pred_clip))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fYdyPnaxCGv"
      },
      "source": [
        "#Step 5: Multimodal Embeddings 2 (Cosine Similarity Classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XubaDyC8xRWc"
      },
      "outputs": [],
      "source": [
        "## Import required libraries\n",
        "\"\"\"from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import PIL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vSQSDNmYxmUu"
      },
      "outputs": [],
      "source": [
        "# Step 5: Multimodal Embeddings 2 (Cosine Similarity Classifier)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from tqdm import tqdm\n",
        "import PIL\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Load CLIP model and processor\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "# Recreate label_name from encoded label if missing\n",
        "if 'label_name' not in df.columns:\n",
        "    reverse_label_map = {v: k for k, v in label_to_index.items()}\n",
        "    df['label_name'] = df['label'].map(reverse_label_map)\n",
        "\n",
        "# Recreate df_binary_original to include original label names\n",
        "binary_classes = ['walking_running', 'standing']\n",
        "df_binary_original = df[df['label_name'].isin(binary_classes)].copy()\n",
        "\n",
        "# Step 1: Generate CLIP image embeddings for df_binary (if not done before)\n",
        "clip_embeddings = []\n",
        "for path in tqdm(df_binary['image_path']):\n",
        "    image = PIL.Image.open(path).convert(\"RGB\")\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        image_feature = clip_model.get_image_features(**inputs)\n",
        "        image_feature = image_feature / image_feature.norm(p=2, dim=-1, keepdim=True)\n",
        "        clip_embeddings.append(image_feature.squeeze().cpu().numpy())\n",
        "\n",
        "df_binary['clip_embedding'] = clip_embeddings\n",
        "\n",
        "# Step 2: Extract image embeddings\n",
        "image_features = np.stack(df_binary['clip_embedding'].values)\n",
        "image_features = image_features / np.linalg.norm(image_features, axis=1, keepdims=True)\n",
        "\n",
        "# Step 3: Generate CLIP Text Embeddings for Labels (safe version)\n",
        "text_labels = list(binary_classes)  # Ensures valid input for tokenizer\n",
        "print(\"Text labels for CLIP encoding:\", text_labels)\n",
        "\n",
        "text_inputs = clip_processor(text=text_labels, return_tensors=\"pt\", padding=True)\n",
        "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = clip_model.get_text_features(**text_inputs)\n",
        "    text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "text_features = text_features.cpu().numpy()\n",
        "\n",
        "# Step 4: Compute cosine similarities between image and text embeddings\n",
        "cos_sim = cosine_similarity(image_features, text_features)\n",
        "pred_indices = np.argmax(cos_sim, axis=1)\n",
        "\n",
        "# Step 5: Evaluate\n",
        "true_labels = df_binary['label'].values.astype(int)\n",
        "pred_labels = pred_indices\n",
        "\n",
        "print(\"\\nClassification Report (Cosine Similarity Classifier):\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot"
      ],
      "metadata": {
        "id": "dm3zkhor8IKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[x for x in globals() if 'history' in x]"
      ],
      "metadata": {
        "id": "IXyyviBu0l7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = {k: v for k, v in accuracies.items() if v is not None}\n",
        "print(accuracies)"
      ],
      "metadata": {
        "id": "rUeOsO_P2duQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# Collect validation accuracies\n",
        "accuracies = {\n",
        "    'CNN_base': history_base.history['val_accuracy'][-1],\n",
        "    'FNN_base': history_fnn.history['val_accuracy'][-1],\n",
        "    'CNN_gen (Full w/ EarlyStopping)': history_gen_aug_all.history['val_accuracy'][-1],\n",
        "    'CLIP Embedding Classifier': history_clip.history['val_accuracy'][-1],\n",
        "    'Transfer Learning (ResNet18)': transfer_val_acc if 'transfer_val_acc' in globals() else None,\n",
        "    'CLIP Cosine Similarity': cosine_val_acc if 'cosine_val_acc' in globals() else None,\n",
        "}\n",
        "\n",
        "# Remove missing entries\n",
        "accuracies = {k: v for k, v in accuracies.items() if v is not None}\n",
        "\n",
        "# X and Y\n",
        "models = list(accuracies.keys())\n",
        "scores = list(accuracies.values())\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(models, scores, color='skyblue')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Comparison of Model Validation Accuracies')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Annotate bars with accuracy values\n",
        "for bar, acc in zip(bars, scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "             f\"{acc:.2f}\", ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1HZtz2Wd8L-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}